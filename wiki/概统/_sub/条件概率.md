### 一、核心思想：在已知条件下更新概率

**1. 直观理解**

想象一下，你正要去郊游，想知道今天下雨的概率。你查了一下历史数据，发现这个季节任意一天下雨的概率是 20%。

- **无条件概率**：\( P (\text{下雨}) = 0.2 \)

现在，你起床后看到天空**乌云密布**。那么，此时你还会认为下雨的概率是 20%吗？显然不会。你知道在“乌云密布”这个**条件**下，下雨的可能性大大增加了。

- **条件概率**：\( P (\text{下雨} | \text{乌云密布}) \) 这个概率可能高达 90%。

这里的竖线 `|` 表示“在... 条件下”，它后面的信息是我们已经知道的、确定的**条件**。

**核心**：条件概率反映了**信息的更新**。当我们获得了新的信息（条件），事件的概率就会随之改变。

---

### 二、定义与公式

**1. 正式定义**

设 A 和 B 是同一个随机试验中的两个事件，且 \( P (B) > 0 \)（即事件 B 已经发生是可能的）。
在事件 B 发生的条件下，事件 A 发生的**条件概率**，记作 \( P (A|B) \)，定义为：
$$
P (A|B) = \frac{P (A \cap B)}{P (B)}
$$

**2. 公式解读**

- \( P (A \cap B) \)：是事件 A **和** B **同时发生**的概率（联合概率）。
- \( P (B) \)：是事件 B 发生的（原始）概率。
- \( P (A|B) \)：可以理解为，在**已经缩小的样本空间 B** 中，事件 A 发生的“比例”。

**3. 文氏图解释**

想象一个矩形代表所有可能结果的样本空间。
- 事件 B 已经发生，意味着我们的世界“缩小”到了代表 B 的圆圈内部。
- 在这个新的、缩小的世界里，A 能发生的部分，就是既在 A 中又在 B 中的部分，即 \( A \cap B \)。
- 所以，在新的世界里，A 发生的概率就是 \( A \cap B \) 的面积占 B 的面积的比例。



---

### 三、经典例题与应用

**例题 1：扑克牌抽牌**

一副标准扑克牌（52 张），无大小王。
- 事件 A：抽到一张 K。
- 事件 B：抽到一张红桃。

A) 求无条件概率 \( P (A) \)：
$$
P (A) = \frac{4}{52} = \frac{1}{13}
$$

B) 如果已知抽到的牌是红桃（事件 B 已发生），求它是 K 的概率 \( P (A|B) \)：
- 方法 1（使用定义）：
  - \( P (A \cap B) \)：抽到的牌既是 K 又是红桃的概率。只有一张**红桃 K**。所以 \( P (A \cap B) = \frac{1}{52} \)。
  - \( P (B) \)：抽到红桃的概率。红桃有 13 张。所以 \( P (B) = \frac{13}{52} = \frac{1}{4} \)。
  - \( P (A|B) = \frac{P (A \cap B)}{P (B)} = \frac{1/52}{13/52} = \frac{1}{13} \)。

- 方法 2（直观理解，缩小样本空间）：
  - 已知条件是“抽到红桃”，那么我们的样本空间就从 52 张牌**缩小**到了 13 张红桃牌。
  - 在这 13 张红桃牌中，只有 1 张是 K。
  - 所以，\( P (A|B) = \frac{1}{13} \)。

**注意**：在这个例子中，\( P (A) = P (A|B) \)。这意味着“抽到 K”的概率不受“抽到红桃”这个信息的影响。我们称事件 A 和 B **相互独立**。

---

**例题 2：医疗检测（贝叶斯定理的引例）**

某种疾病在人群中的患病率为 1%。针对该疾病的检测，其准确率为 99%（即患病者检测呈阳性的概率为 99%，健康人检测呈阴性的概率也为 99%）。

问：如果一个人检测结果为阳性，他真正患病的概率是多少？

**定义事件**：
- \( D \)：一个人实际患病。 \( P (D) = 0.01 \)
- \( T^+ \)：检测结果为阳性。

**已知条件概率**：
- \( P (T^+|D) = 0.99 \) （真阳性率，灵敏度）
- \( P (T^-|\text{非}D) = 0.99 \) → \( P (T^+|\text{非}D) = 0.01 \) （假阳性率）

**我们需要求**：\( P (D|T^+) \) （在检测为阳性的条件下，真正患病的概率）

**计算**：
1.  先求检测为阳性的总概率 \( P (T^+) \)。
    - 真阳性：\( P (T^+ \cap D) = P (T^+|D) \times P (D) = 0.99 \times 0.01 = 0.0099 \)
    - 假阳性：\( P (T^+ \cap \text{非}D) = P (T^+|\text{非}D) \times P (\text{非}D) = 0.01 \times 0.99 = 0.0099 \)
    - 所以，\( P (T^+) = 0.0099 + 0.0099 = 0.0198 \)

2.  应用条件概率公式：
    $$
    P (D|T^+) = \frac{P (D \cap T^+)}{P (T^+)} = \frac{0.0099}{0.0198} = 0.5
    $$

**结论**：即使检测准确率高达 99%，一个检测结果为阳性的人，其真正患病的概率也只有**50%**！这个反直觉的结果凸显了条件概率和先验概率（患病率）的重要性。这个例子直接引出了强大的**贝叶斯定理**。

---

### 四、乘法公式

由条件概率的定义 \( P (A|B) = \frac{P (A \cap B)}{P (B)} \)，我们可以立即得到一个极其有用的公式——**乘法公式**：

$$
P (A \cap B) = P (A|B) \cdot P (B)
$$
或者等价地：
$$
P (A \cap B) = P (B|A) \cdot P (A)
$$

**解释**：两个事件**同时发生**的概率，等于其中一个事件发生的概率，乘以在该事件发生的条件下另一个事件发生的概率。

**推广到多个事件**：
$$
P (A \cap B \cap C) = P (A) \cdot P (B|A) \cdot P (C|A \cap B)
$$
这个公式在计算复杂事件的概率时非常有用，尤其是涉及** sequential**（顺序）过程时。

**例子**：一个盒子里有 5 个红球和 3 个蓝球，连续抽取 2 个，**不放回**。求两次都抽到红球的概率。
- 事件 A：第一次抽到红球。 \( P (A) = \frac{5}{8} \)
- 事件 B：第二次抽到红球。
- 在 A 发生的条件下（即第一次已拿走一个红球），盒中剩 4 红 3 蓝，所以 \( P (B|A) = \frac{4}{7} \)
- 根据乘法公式：\( P (A \cap B) = P (A) \cdot P (B|A) = \frac{5}{8} \times \frac{4}{7} = \frac{5}{14} \)

---

### 五、独立性

**1. 定义**
如果事件 B 的发生不影响事件 A 发生的概率（反之亦然），则称事件 A 和 B **相互独立**。用公式表示为：
$$
P (A|B) = P (A)
$$
或者，更常用的是：
$$
P (A \cap B) = P (A) \cdot P (B)
$$
（当 \( P (A)>0, P (B)>0 \) 时，这两个定义是等价的。）

**2. 理解**
独立性意味着两个事件之间**没有因果关系或逻辑关联**。知道其中一个发生了，并不会给我们任何关于另一个是否会发生的信息。

**例子**：
- 抛一枚公平的硬币两次。“第一次是正面”和“第二次是正面”是独立的。
- 但是，“今天下雨”和“街上有人打伞”是**不独立**的，因为下雨会导致打伞的概率增加。
/
**注意**：**互斥**（\( A \cap B = \emptyset \)）和**独立**是两个完全不同的概念。事实上，如果 \( P (A)>0 \) 且 \( P (B)>0 \)，那么互斥事件**一定不独立**！因为如果 A 发生，B 就一定不发生，即 \( P (B|A) = 0 \neq P (B) \)。

---

### 六、贝叶斯定理

这是条件概率皇冠上的明珠，它描述了如何根据新的证据来更新我们对某个假设的信念。

**定理公式**：
$$
P (A|B) = \frac{P (B|A) \cdot P (A)}{P (B)}
$$
其中：
- \( P (A|B) \) 称为**后验概率**：在得到证据 B 后，对事件 A 发生概率的更新估计。
- \( P (A) \) 称为**先验概率**：在得到任何证据之前，对事件 A 发生概率的初始信念。
- \( P (B|A) \) 称为**似然度**：如果 A 为真，观察到证据 B 的可能性有多大。
- \( P (B) \) 是证据 B 发生的总概率，通常用全概率公式计算：\( P (B) = P (B|A) P (A) + P (B|\text{非}A) P (\text{非}A) \)。

**回顾之前的医疗检测例子**，我们实际上就是不自觉地使用了贝叶斯定理：
- \( A \) 是 \( D \)（患病）
- \( B \) 是 \( T^+ \)（检测阳性）
$$
P (D|T^+) = \frac{P (T^+|D) \cdot P (D)}{P (T^+)} = \frac{0.99 \times 0.01}{0.0198} = 0.5
$$

### 总结

- **条件概率** \( P (A|B) \) 是概率论的核心，它体现了**信息更新**的思想。
- 其定义公式 \( P (A|B) = \frac{P (A \cap B)}{P (B)} \) 是所有推导的基石。
- **乘法公式** \( P (A \cap B) = P (A|B) P (B) \) 是计算交事件概率的强大工具。
- **独立性** \( P (A \cap B) = P (A) P (B) \) 描述了两个事件互不影响的情形。
- **贝叶斯定理** \( P (A|B) = \frac{P (B|A) P (A)}{P (B)} \) 提供了一个系统性的框架，用于根据新证据来修正我们的认知，在统计学、机器学习、医学、法学等领域有极其广泛的应用。